{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch, GoogleScholarSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key():\n",
    "    with open('key.txt') as f:\n",
    "        return f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get 2 pages\n",
    "#start = 0\n",
    "#end = 40\n",
    "#page_size = 10\n",
    "\n",
    "params = {\n",
    "  \"api_key\": get_key(),\n",
    "  \"engine\": \"google_scholar\",\n",
    "  \"start\": \"0\",\n",
    "  \"as_ylo\": \"1990\",\n",
    "  \"q\": \"mezcal\",\n",
    "  \"as_sdt\": \"0\",\n",
    "  \"scisbd\": \"1\"\n",
    "#  \"start\": start,\n",
    "#  \"end\": end,\n",
    "#  \"num\": page_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize SerpApi search\n",
    "search = GoogleSearch(params)\n",
    "data = search.get_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a python generator using parameter\n",
    "pages = search.pagination()\n",
    "# or set custom parameter\n",
    "# pages = search.pagination(start, end, page_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search.get_dict()\n",
    "    if \"error\" in result:\n",
    "        print(\"oops error: \", result[\"error\"])\n",
    "        continue\n",
    "    result['search_metadata'])\n",
    "    # add search to the search_queue\n",
    "    search_queue.put(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as proof of concept \n",
    "# urls collects\n",
    "\n",
    "urls = []\n",
    "\n",
    "\n",
    "# fetch one search result per iteration \n",
    "for page in pages:\n",
    "  print(f\"Current page: {page['serpapi_pagination']['current']}\")\n",
    "  for news_result in page[\"news_results\"]:\n",
    "    print(f\"Title: {news_result['title']}\\nLink: {news_result['link']}\\n\")\n",
    "    urls.append(news_result['link'])\n",
    "  \n",
    "# check if the total number pages is as expected\n",
    "# note: the exact number if variable depending on the search engine backend\n",
    "if len(urls) == (end - start):\n",
    "  print(\"all search results count match!\")\n",
    "if len(urls) == len(set(urls)):\n",
    "  print(\"all search results are unique!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# loop through a list of companies\n",
    "for company in ['amd', 'nvidia', 'intel']:\n",
    "    print(\"execute async search: q = \" + company)\n",
    "    search.params_dict[\"q\"] = company\n",
    "    result = search.get_dict()\n",
    "    if \"error\" in result:\n",
    "        print(\"oops error: \", result[\"error\"])\n",
    "        continue\n",
    "    print(\"add search to the queue where id: \", result['search_metadata'])\n",
    "    # add search to the search_queue\n",
    "    search_queue.put(result)\n",
    "\n",
    "print(\"wait until all search statuses are cached or success\")\n",
    "\n",
    "# Create regular search\n",
    "while not search_queue.empty():\n",
    "    result = search_queue.get()\n",
    "    search_id = result['search_metadata']['id']\n",
    "\n",
    "    # retrieve search from the archive - blocker\n",
    "    print(search_id + \": get search from archive\")\n",
    "    search_archived = search.get_search_archive(search_id)\n",
    "    print(search_id + \": status = \" +\n",
    "          search_archived['search_metadata']['status'])\n",
    "\n",
    "    # check status\n",
    "    if re.search('Cached|Success',\n",
    "                 search_archived['search_metadata']['status']):\n",
    "        print(search_id + \": search done with q = \" +\n",
    "              search_archived['search_parameters']['q'])\n",
    "    else:\n",
    "        # requeue search_queue\n",
    "        print(search_id + \": requeue search\")\n",
    "        search_queue.put(result)\n",
    "\n",
    "        # wait 1s\n",
    "        time.sleep(1)\n",
    "\n",
    "print('all searches completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
